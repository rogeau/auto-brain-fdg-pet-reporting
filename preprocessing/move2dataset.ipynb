{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f39154d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7580f7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select an Excel file from the list:\n",
      "1: 20230831-20240831.xlsx\n",
      "Excel file selected: raw_reports/20230831-20240831.xlsx\n"
     ]
    }
   ],
   "source": [
    "nifti_folder = Path(\"nifti\")\n",
    "excel_folder = Path(\"raw_reports\")\n",
    "\n",
    "excel_files = list(excel_folder.glob(\"*.xlsx\"))\n",
    "\n",
    "if not excel_files:\n",
    "    print(f\"No Excel files found in {excel_folder.resolve()}\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"Select an Excel file from the list:\")\n",
    "for i, file_path in enumerate(excel_files, 1):\n",
    "    print(f\"{i}: {file_path.name}\")\n",
    "while True:\n",
    "    choice = input(f\"Enter the number (1-{len(excel_files)}): \").strip()\n",
    "    if choice.isdigit():\n",
    "        idx = int(choice)\n",
    "        if 1 <= idx <= len(excel_files):\n",
    "            excel_path = excel_files[idx - 1]\n",
    "            break\n",
    "    print(\"Invalid choice, please try again.\")\n",
    "\n",
    "print(f\"Excel file selected: {excel_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64705e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_340747/2343179547.py:3: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  df['Date'] = pd.to_datetime(df['Date prévue'], dayfirst=True, errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(excel_path, dtype=str)\n",
    "df['N° IPP'] = df['N° IPP'].str.extract(r'(\\d+)') # Remove any extra characters\n",
    "df['Date'] = pd.to_datetime(df['Date prévue'], dayfirst=True, errors='coerce')\n",
    "df['Date'] = df['Date'].dt.strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b245426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove absent nifti / excel pairs\n",
    "valid_pairs = set(zip(df['N° IPP'], df['Date']))\n",
    "\n",
    "nii_files = list(nifti_folder.rglob(\"*.nii.gz\"))\n",
    "file_map = {}  # (ipp, date) -> full file path(s)\n",
    "for f in nii_files:\n",
    "    base = f.name[:-7]\n",
    "    match = re.match(r\"(\\d+)_([0-9]{8})_.*\", base)\n",
    "    if not match:\n",
    "        print(f\"Skipping malformed file: {f.name}\")\n",
    "        continue\n",
    "    ipp, date = match.groups()\n",
    "    file_map.setdefault((ipp, date), []).append(f)\n",
    "\n",
    "file_pairs = set(file_map.keys())\n",
    "\n",
    "df_filtered = df[df.apply(lambda row: (row['N° IPP'], row['Date']) in file_pairs, axis=1)]\n",
    "\n",
    "unmatched_files = [f for (ipp, date), paths in file_map.items()\n",
    "                   if (ipp, date) not in valid_pairs\n",
    "                   for f in paths]\n",
    "\n",
    "for f in unmatched_files:\n",
    "    print(f\"Deleting unmatched file: {f}\")\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d46bc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check or create IPP -> ID lookup table\n",
    "lookup_df = df_filtered[['N° IPP']].copy()\n",
    "lookup_df = lookup_df.rename(columns={\"N° IPP\": \"IPP\"})\n",
    "\n",
    "lookup_path = Path(\"lookup_table.xlsx\")\n",
    "\n",
    "if lookup_path.exists():\n",
    "    existing_df = pd.read_excel(lookup_path)\n",
    "    combined_df = pd.concat([existing_df, lookup_df], ignore_index=True)\n",
    "else:\n",
    "    combined_df = lookup_df\n",
    "\n",
    "combined_df = combined_df.drop_duplicates(subset=[\"IPP\"], keep=\"first\")\n",
    "\n",
    "\n",
    "if 'SubjectID' not in combined_df.columns:\n",
    "    combined_df['SubjectID'] = None\n",
    "\n",
    "missing_mask = combined_df['SubjectID'].isna() | (combined_df['SubjectID'] == \"\")\n",
    "\n",
    "existing_ids = combined_df['SubjectID'].dropna().unique()\n",
    "existing_numbers = [int(sid.replace(\"SUB\", \"\")) for sid in existing_ids if sid.startswith(\"SUB\") and sid[3:].isdigit()]\n",
    "next_number = max(existing_numbers, default=0) + 1\n",
    "num_missing = missing_mask.sum()\n",
    "new_ids = [f\"SUB{next_number + i:06d}\" for i in range(num_missing)]\n",
    "combined_df.loc[missing_mask, 'SubjectID'] = new_ids\n",
    "\n",
    "combined_df.to_excel(lookup_path, index=False)\n",
    "print(f\"Lookup table saved to {lookup_path}\")\n",
    "\n",
    "\n",
    "df_filtered = df_filtered.merge(\n",
    "    combined_df[['IPP', 'SubjectID']],\n",
    "    how='left',\n",
    "    left_on='N° IPP',\n",
    "    right_on='IPP'\n",
    ")\n",
    "\n",
    "df_filtered = df_filtered.drop(columns=['IPP'])\n",
    "\n",
    "output_excel = excel_path.with_name(excel_path.stem + \"_filtered.xlsx\")\n",
    "df_filtered.to_excel(output_excel, index=False)\n",
    "print(f\"Filtered Excel saved to {output_excel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to dataset folder\n",
    "output_base = Path(\"dataset\")\n",
    "output_base.mkdir(exist_ok=True)\n",
    "\n",
    "ipp_date_to_subj = {\n",
    "    (row['N° IPP'], row['Date']): row['SubjectID']\n",
    "    for _, row in df_filtered.iterrows()\n",
    "}\n",
    "\n",
    "for (ipp, date), files in file_map.items():\n",
    "    subject_id = ipp_date_to_subj.get((ipp, date))\n",
    "    if not subject_id:\n",
    "        continue\n",
    "\n",
    "    for filepath in files:\n",
    "        # Extract the part after the second underscore for filename\n",
    "        base = filepath.name[:-7]  # Remove '.nii.gz'\n",
    "        parts = base.split('_', 2)  # Split max 3 parts\n",
    "        if len(parts) < 3:\n",
    "            print(f\"Warning: filename '{filepath.name}' has fewer than 3 parts, skipping\")\n",
    "            continue\n",
    "        rest_filename = parts[2] + \".nii.gz\"\n",
    "\n",
    "        # Construct new path: dataset/SubjectID/date/rest_filename\n",
    "        new_dir = output_base / subject_id / date\n",
    "        new_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        new_path = new_dir / rest_filename\n",
    "\n",
    "        print(f\"Moving {filepath} -> {new_path}\")\n",
    "        shutil.move(str(filepath), str(new_path))\n",
    "\n",
    "shutil.rmtree(nifti_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d04a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    return str(x).replace('\\n', ' ').replace('\\r', ' ').strip()\n",
    "\n",
    "json_path = \"dataset/reports.json\"\n",
    "\n",
    "# Prepare new data\n",
    "json_data = []\n",
    "for _, row in df_filtered.iterrows():\n",
    "    record = {\n",
    "        \"SubjectID\": clean_text(row[\"SubjectID\"]),\n",
    "        \"Date\": clean_text(row[\"Date\"]),\n",
    "        \"Indication\": clean_text(row.get(\"Indication\")),\n",
    "        \"PET_results\": clean_text(row.get(\"Resultat_TEP\")),\n",
    "        \"MRI_results\": clean_text(row.get(\"Resultat_IRM\")),\n",
    "        \"Conclusion\": clean_text(row.get(\"Conclusion\"))\n",
    "    }\n",
    "    json_data.append(record)\n",
    "\n",
    "# Load existing data if file exists\n",
    "if os.path.exists(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            existing_data = json.load(f)\n",
    "            if not isinstance(existing_data, list):\n",
    "                existing_data = []\n",
    "        except json.JSONDecodeError:\n",
    "            existing_data = []\n",
    "else:\n",
    "    existing_data = []\n",
    "\n",
    "# Append new data\n",
    "combined_data = existing_data + json_data\n",
    "\n",
    "# Save back to JSON\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(combined_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"JSON data appended to {json_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "morpheus3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
